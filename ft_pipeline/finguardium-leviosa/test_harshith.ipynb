{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "from groq import Groq\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import HTML\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "\n",
    "\n",
    "subscription_key = \"4eb7140d9eff41df888a10edb9339a33\"\n",
    "search_url = \"https://api.bing.microsoft.com/v7.0/news/search\"\n",
    "\n",
    "\n",
    "API_KEY = \"AIzaSyDMXlOxb2PPlLJofl0sqEzBGVJPfwLstOA\"\n",
    "SEARCH_ENGINE_ID = \"e2fd271d0f6d641d3\"\n",
    "\n",
    "YOUR_API_KEY = \"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\"\n",
    "\n",
    "ppx_client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bing_search_for_src(test_labels, labels_grounded = {}):\n",
    "#     labels_grounded = {}\n",
    "    \n",
    "#     return labels_grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def malicious_ques_gen(oa_client, labels_grounded):\n",
    "#     malicious_questions = {}\n",
    "    \n",
    "#     return malicious_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def step_2(oai_client, labels):\n",
    "#     # final_labels = step1(oai_client, rest_client_conf, pg_wise_data, vector_db)\n",
    "#     labels_grounded = bing_search_for_src(test_labels, labels_grounded ={})\n",
    "#     malicious_questions = malicious_ques_gen(oa_client, labels_grounded)\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heading(text_content):\n",
    "    system_prompt = \"\"\"  \n",
    "        You are a text summary title generation tool in the finance domain, designed to generate very sharp and consise headings based on detailed text content.\n",
    "        \n",
    "        Input Details:  \n",
    "        1. Text content provided by the user  \n",
    "        \n",
    "        Task: Your task is to generate very short and sharp heading based on the provided text content. I will be searching this heading on google. I should get relevant news articles.\n",
    "        \"\"\"\n",
    "    \n",
    "    ideal_output = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": \"\"\"One example of SEBI taking action against a company for market manipulation violations is the case of BPL Limited. In this case, SEBI investigated and found that BPL Limited had indulged in violating regulation 4(a) and (d) of the 1995 Regulations, which prohibit fraudulent and unfair trade practices. The investigation revealed that the company had created a false market and manipulated the prices of its scrip in connivance with Harshad Mehta by aiding, abetting, and being instrumental in effecting transactions. SEBI issued show cause notices to the company and its officers/directors, asking them to explain their conduct. After adjudicating the show cause notice, SEBI confirmed the charges and passed an order directing the company to cease and desist from accessing the capital market for a period of three years.\n",
    "This case demonstrates SEBI's efforts to prevent market manipulation and protect the interests of investors in the Indian securities market. SEBI's actions in this case are in line with its mandate to regulate the securities market and prevent fraudulent and unfair trade practices.\"\"\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"SEBI vs Samir Arora: SEBI takes action against HDFC Bank \"},\n",
    "        {\"role\": \"user\", \"content\": text_content}\n",
    "        ]\n",
    "\n",
    "    client = Groq(api_key='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')\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=ideal_output,\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        top_p=1\n",
    "    )\n",
    "    title_text = completion.choices[0].message.content\n",
    "    return title_text\n",
    "\n",
    "# returns url and desp\n",
    "def bing_engine(ques):\n",
    "    ques = \"\"\"Samir Arora**: SEBI took action against Samir Arora\"\"\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "    params  = {\"q\": ques, \"textDecorations\": True, \"textFormat\": \"HTML\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()  # Parse JSON response into a dictionary\n",
    "\n",
    "    # print(search_results)\n",
    "    urls = [article[\"url\"] for article in search_results[\"value\"]]\n",
    "    # print(urls)\n",
    "\n",
    "    description = [article[\"description\"] for article in search_results[\"value\"]]\n",
    "    # print(description)\n",
    "    # rows = \"\\n\".join([\"<tr><td>{0}</td></tr>\".format(desc) for desc in descriptions])\n",
    "\n",
    "    # html_content = HTML(\"<table>\"+rows+\"</table>\")\n",
    "    # html_content\n",
    "    return urls \n",
    "\n",
    "#returns urls and titles\n",
    "def google_engine(ques):\n",
    "    base_url = f\"https://www.googleapis.com/customsearch/v1?key={API_KEY}&cx={SEARCH_ENGINE_ID}&q={ques}\"\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        data = response.json()\n",
    "        return [item[\"link\"] for item in data.get(\"items\", [])]\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(f\"HTTP Error: {errh}\")\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    return [] \n",
    "\n",
    "def blocklist(used_examples, heading_test):\n",
    "    used_examples.append(heading_test)\n",
    "    print(f'added {heading_test} to blocklist')\n",
    "\n",
    "\n",
    "def is_valid_url(link):\n",
    "    \"\"\"Checks if the link is a valid URL with http or https scheme.\"\"\"\n",
    "    return link.startswith(\"https://\") or link.startswith(\"http://\")\n",
    "\n",
    "def is_genuine_domain(url):\n",
    "    \"\"\"Checks if the URL's domain ends with '.com' and excludes certain domains.\"\"\"\n",
    "    excluded_domains = {'facebook', 'youtube', 'linkedin', 'apps', 'play', 'youtube', 'twitter'}\n",
    "    # excluded_domains = {'facebook', 'youtube', 'linkedin'}\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.split('.')[1] if parsed_url.netloc.startswith('www.') else parsed_url.netloc.split('.')[0]\n",
    "    if domain in excluded_domains:\n",
    "        return False\n",
    "    return parsed_url.netloc.endswith('.com')\n",
    "\n",
    "def filter_genuine_links(links):\n",
    "    \"\"\"Filters links that are both valid and have a trusted TLD (e.g., '.com').\"\"\"\n",
    "    genuine_links = []\n",
    "    for link in links:\n",
    "        if is_valid_url(link) and is_genuine_domain(link):\n",
    "            genuine_links.append(link)\n",
    "    return genuine_links\n",
    "\n",
    "def filter_text(text):\n",
    "    text_no_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', text_no_urls)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text_no_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA]))+', '', text)\n",
    "    # Remove non-ASCII characters\n",
    "    text_no_non_ascii = re.sub(r'[^\\x00-\\x7F]+', '', text_no_urls)\n",
    "    # Retain words, numbers, tabs, and newlines\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text_no_non_ascii)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_dict(data):\n",
    "    for src, nested_dict in data.items():\n",
    "        if isinstance(nested_dict, dict):\n",
    "            for key, text in nested_dict.items():\n",
    "                if isinstance(text, str):\n",
    "                    nested_dict[key] = clean_text(text)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "    pdf_document = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    print('pdf read:200')\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "def text_builder(urls):\n",
    "    \"\"\"Generate a schema where each source maps to a dictionary containing the index and its full concatenated text.\"\"\"\n",
    "    extracted_text = {}\n",
    "    clean_extracted_text = {}\n",
    "\n",
    "    for index, link in enumerate(urls, start=1):\n",
    "        try:\n",
    "            response = requests.head(link)\n",
    "            response.raise_for_status()\n",
    "            content_type = response.headers.get('Content-Type')\n",
    "            \n",
    "            if content_type and 'pdf' in content_type.lower():\n",
    "                combined_link_text = extract_pdf_text(link)\n",
    "            else:\n",
    "                response = requests.get(link)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                texts = list(soup.stripped_strings)\n",
    "                combined_link_text = ' '.join(texts)\n",
    "            \n",
    "            # Store the index as a sub-key inside the source dictionary\n",
    "            if link not in extracted_text:\n",
    "                extracted_text[link] = {}\n",
    "            extracted_text[link][index] = combined_link_text\n",
    "\n",
    "            print(f'Text from {link} ...done')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {link}: {e}\")\n",
    "            continue\n",
    "\n",
    "    clean_extracted_text = clean_dict(extracted_text)\n",
    "    return clean_extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.cnn.com/2001/BUSINESS/asia/04/20/india.sebi/index.html',\n",
       " 'https://www.state.gov/wp-content/uploads/2020/03/Tab-2-INCSR-Vol-2-508.pdf',\n",
       " 'https://www.scribd.com/document/135879998/Harshad-mehta']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = \"SEBI Cracks Down on Market Manipulation: BPL Limited Case\"\n",
    "val = google_engine(ques)        \n",
    "val[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_info(total_store, label_name, case_studies, link_text):\n",
    "    if label_name not in total_store:\n",
    "        total_store[label_name] = {\n",
    "            'case_studies': [case_studies],  # Wrap in list if multiple entries expected\n",
    "            'link_text': [link_text]  # Wrap in list if multiple entries expected\n",
    "        }\n",
    "    else:\n",
    "        total_store[label_name]['case_studies'].append(case_studies)\n",
    "        total_store[label_name]['link_text'].append(link_text)\n",
    "    return total_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_examples = 'SEBI Cracks Down on Videocon International for Market Manipulation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_summary(ppx_client, label,used_examples):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            f\"\"\"You are a finance consultant tasked with providing a strong and detailed case study of SEBI enforcement actions against companies/entities for a specific violation. \n",
    "            Task: \n",
    "            Create a comprehensive, highly technical summary of SEBI's action.\n",
    "\n",
    "            Strict Rules:\n",
    "            1. DO NOT use any examples from here: {str(used_examples)}. \n",
    "\n",
    "            Instructions:\n",
    "            1. Focus on one specific violation: {label}.\n",
    "            2. Deliver one detailed 400-token summary of SEBI's action for the specified violation.\n",
    "            3. Ensure the summary is accurate and highly technical, with no fabricated information.\n",
    "            4. Select a strong example that effectively illustrates SEBI's regulatory impact.\n",
    "            \"\"\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Provide a strong example of SEBI taking action against a company for {label} violations.\"\n",
    "        ),\n",
    "    },  \n",
    "]\n",
    "\n",
    "    # print(messages)\n",
    "\n",
    "    response = ppx_client.chat.completions.create(\n",
    "        model=\"llama-3-sonar-small-32k-online\",\n",
    "        messages=messages,\n",
    "        max_tokens = 1024,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_example_summary(ppx_client, 'Market Manipulations', used_examples)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def run_sample_pipeline(test_labels_new2):\n",
    "    test_labels_new2\n",
    "    final_store = {}\n",
    "    used_examples = []\n",
    "    final_backup= {}\n",
    "    support_links = []\n",
    "\n",
    "    for lab in test_labels_new2:\n",
    "        print(f'starting for {lab}')\n",
    "        for case in range(0,5):\n",
    "        # k is the number of case studis we want. \n",
    "            support_links =[]\n",
    "            src_links = []\n",
    "            case_summary = get_example_summary(ppx_client, lab, used_examples)\n",
    "            time.sleep(1)\n",
    "            case_heading = get_heading(case_summary)\n",
    "            used_examples.append(case_heading)\n",
    "            support_links = google_engine(case_heading)\n",
    "            if support_links == []:\n",
    "                support_links = bing_engine(case_heading)\n",
    "            src_links = support_links[:3]\n",
    "            text_content = text_builder(src_links)\n",
    "            print('text_content build success')\n",
    "            # Update final_store directly inside thes loop\n",
    "            final_store = store_info(final_store, lab,case_summary,text_content)\n",
    "            final_backup.update(final_store)\n",
    "    return final_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pdfs_filter.json', 'w') as f:\n",
    "    json.dump(final_backup, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEBI Fines Reliance Industries $210M for Insider Trading',\n",
       " 'SEBI Cracks Down on Market Manipulation: Samir Arora Case',\n",
       " 'SEBI Cracks Down on Samir Arora: Insider Trading and Fraudulent Practices Exposed',\n",
       " 'SEBI Cracks Down on Market Manipulation: BPL Limited Fined Rs 10 Crore',\n",
       " 'SEBI Cracks Down on Sterlite Industries for Market Manipulation']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_backup['Market manipulation']['link_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "You are working on a finance company looking to fine tune an LLM guard model for financial services. You have been given a circular issued by SEBI (financial regulator in India) related to registered investment advisors.\n",
    "\n",
    "Answer Instructions:\n",
    "0. DONOT USE THE WEB\n",
    "1. Only cover guidelines related to investment advice offered.\n",
    "2. You need to extract as a python list the set of labels corresponding to bad behavior as per regulator. \n",
    "3. LABELS SHOULD BE 3-4 WORDS MAX\n",
    "4. Mention it in adversarial tone. For example: (default) Buy TATA stock when you are selling TATA stock -> (required) Conflict of Interest \n",
    "5. Don't reverse double negatives. example: (default) Presence of Bias in research -> (wrong flip) Lack of Presence of Bias in Research\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "labels_list = [label1, label2, ...]\n",
    "```\"\"\"\n",
    "        } ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

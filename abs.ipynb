{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U mongoengine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from openai import OpenAI\n",
    "import mongoengine\n",
    "import weaviate\n",
    "import threading\n",
    "\n",
    "\n",
    "openAIClient_novis = AzureOpenAI(\n",
    "  api_key = \"add2ae8844844d55bd3e1300ccbc9bc2\",  \n",
    "  api_version = \"2023-05-15\",\n",
    "  azure_endpoint = \"https://openai-service-onfi.openai.azure.com/\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mongoengine\n",
    "from mongoengine import Document, StringField, IntField, connect, DynamicField\n",
    "\n",
    "class NSE(Document):\n",
    "    page_no = IntField()\n",
    "    doc_name = StringField()\n",
    "    # question = StringField()  # Uncomment this if you want to use it\n",
    "    answer = StringField()\n",
    "    index_field = DynamicField()\n",
    "    meta = {\n",
    "        'collection': 'nse_full',\n",
    "        'indexes': [\n",
    "            {\n",
    "                'fields': ['doc_name', '$answer'],\n",
    "                'default_language': 'english',\n",
    "            }\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshu/onfi/nse/nse-env/lib/python3.10/site-packages/mongoengine/connection.py:199: DeprecationWarning: No uuidRepresentation is specified! Falling back to 'pythonLegacy' which is the default for pymongo 3.x. For compatibility with other MongoDB drivers this should be specified as 'standard' or '{java,csharp}Legacy' to work with older drivers in those languages. This will be changed to 'unspecified' in a future release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['cluster2-shard-00-01.smsjk.mongodb.net:27017', 'cluster2-shard-00-02.smsjk.mongodb.net:27017', 'cluster2-shard-00-00.smsjk.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', authsource='admin', replicaset='atlas-ajd6zf-shard-0', tls=True, read_preference=Primary(), uuidrepresentation=3, driver=DriverInfo(name='MongoEngine', version='0.28.2', platform=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = \"mongodb+srv://nse_read:O4LZN0OtBGFHtfgh@cluster2.smsjk.mongodb.net/onfi_dev_atlas_db?retryWrites=true&w=majority\"\n",
    "connect(host=uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_name_lol = 'VOLTAS.pdf'\n",
    "# docs_test = NSE.objects(doc_name=test_doc_name_lol).all()\n",
    "# print(docs_test)\n",
    "# company_docs_test = {}\n",
    "\n",
    "# for doc in docs_test:\n",
    "#     if doc.doc_name not in company_docs_test:\n",
    "#         company_docs_test[doc.doc_name] = {}\n",
    "#     company_docs_test[doc.doc_name][doc.page_no] = doc.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_docs_test['VOLTAS.pdf'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_docs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_csv(matched_chunks, keyword, filename='output.csv'):\n",
    "#     with open(filename, mode='w', newline='') as csvfile:\n",
    "#         csv_writer = csv.writer(csvfile)\n",
    "#         csv_writer.writerow(['ticker', 'keyword', 'pg_no', 'summary (page_wise)'])\n",
    "#         summary = summaries_builder(matched_chunks, keyword)\n",
    "#         csv_writer.writerow([ticker, keyword, pg_no, summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "# from elasticsearch.helpers import bulk\n",
    "# es = Elasticsearch(\n",
    "#     [\"https://d9be14c12e6344519c93ef6a4a6f009b.us-central1.gcp.cloud.es.io:443\"],\n",
    "#     api_key=\"TXJZV29JOEJibXRCaTVqQWgycUY6RkJPZWlNZkRSRTJUOFlGNGQ1LV9FUQ==\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'instance-0000000000', 'cluster_name': '582a2f1e2d7d4046b16a0bad619b7536', 'cluster_uuid': 'DL2vxmcZTDWvYm1NeO6Djg', 'version': {'number': '8.13.4', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'da95df118650b55a500dcc181889ac35c6d8da7c', 'build_date': '2024-05-06T22:04:45.107454559Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Correct configuration with cloud_id and api_key\n",
    "es = Elasticsearch(\n",
    "    cloud_id=\"a50aa60c9d4241b0941ae811a887a5d1:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJDU4MmEyZjFlMmQ3ZDQwNDZiMTZhMGJhZDYxOWI3NTM2JDMwMTQxMGZhMjg1YjQxMjU4ZThmM2IzZTIwY2M1YjA0\",\n",
    "    api_key=\"X1k0YnA0OEJkY0RlOHNzQUl3aEg6MENUeEwxc1dTU0tBRlFQdW9FdWpyUQ==\"\n",
    ")\n",
    "\n",
    "# Example query to check connection\n",
    "try:\n",
    "    response = es.info()\n",
    "    print(response){'name': 'instance-0000000000', 'cluster_name': '582a2f1e2d7d4046b16a0bad619b7536', 'cluster_uuid': 'DL2vxmcZTDWvYm1NeO6Djg', 'version': {'number': '8.13.4', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'da95df118650b55a500dcc181889ac35c6d8da7c', 'build_date': '2024-05-06T22:04:45.107454559Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Elasticsearch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import time\n",
    "\n",
    "# Elasticsearch client configuration\n",
    "# es = Elasticsearch(\n",
    "#     cloud_id=\"a50aa60c9d4241b0941ae811a887a5d1:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJDU4MmEyZjFlMmQ3ZDQwNDZiMTZhMGJhZDYxOWI3NTM2JDMwMTQxMGZhMjg1YjQxMjU4ZThmM2IzZTIwY2M1YjA0\",\n",
    "#     api_key=\"TLW80RXA0OEJkY0RlOHNzQWd3aFg6NHBZTTZka3dRMksxRk9sSy16TjhlUQ==\"\n",
    "# )\n",
    "\n",
    "def get_similar_chunks(document_name, keywords):\n",
    "    actions = []\n",
    "    matched_chunks = {}\n",
    "    final_matched_chunks = {}\n",
    "    company_information = {}\n",
    "\n",
    "    docs_test = NSE.objects(doc_name=document_name).all()\n",
    "    for doc in docs_test:\n",
    "        if doc.doc_name not in company_information:\n",
    "            company_information[doc.doc_name] = {}\n",
    "        company_information[doc.doc_name][doc.page_no] = doc.answer\n",
    "\n",
    "    for filename, content_dict in company_information.items():\n",
    "        for slide_number, text in content_dict.items():\n",
    "            action = {\n",
    "                \"_index\": \"documents\",\n",
    "                \"_id\": f\"{filename}_{slide_number}\",\n",
    "                \"_source\": {\n",
    "                    \"content\": text,\n",
    "                    \"document_name\": filename,\n",
    "                    \"page_number\": slide_number\n",
    "                }\n",
    "            }\n",
    "            actions.append(action)\n",
    "\n",
    "    # Perform bulk indexing with retry mechanism\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if actions:\n",
    "                bulk(es, actions)\n",
    "            break  # If bulk indexing is successful, exit the loop\n",
    "        except Exception as e:\n",
    "            print(f\"Bulk indexing failed (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    # Perform search query with retry mechanism\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = es.search(index=\"documents\", body={\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\"match\": {\"content\": keywords}},\n",
    "                            {\"match\": {\"document_name\": document_name}}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "            break  # If search is successful, exit the loop\n",
    "        except Exception as e:\n",
    "            print(f\"Search query failed (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    print('got result')\n",
    "    if not result['hits']['hits']:\n",
    "        print('None found!')\n",
    "    else:\n",
    "        for hit in result['hits']['hits']:\n",
    "            doc_name = hit['_source']['document_name']\n",
    "            page_no = hit['_source']['page_number']\n",
    "            content = hit['_source']['content']\n",
    "\n",
    "            if doc_name not in matched_chunks:\n",
    "                matched_chunks[doc_name] = {}\n",
    "            matched_chunks[doc_name][page_no] = content\n",
    "\n",
    "    final_matched_chunks.update(matched_chunks)\n",
    "\n",
    "    return final_matched_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got result\n"
     ]
    }
   ],
   "source": [
    "matched_chunked_test = get_similar_chunks('TATAMOTORS.pdf', 'E-Bike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([497, 498, 496, 501, 495, 500, 14, 499, 106, 459])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_chunked_test['TATAMOTORS.pdf'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_chunked_test['TATAMOTORS.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_summary_pagewise(content, ticker, keyword):\n",
    "    systemPrompt_qmake = \"\"\"\n",
    "    You are a helpful AI-based expert specialized in annual reports of top 50 NSE companies in India. \n",
    "    You will be given an OCR-generated output from a annual report of {section_name}.\n",
    "    Input details:\n",
    "    1. Excerpt from their documents\n",
    "     \n",
    "    Your task is to generate a 800-token summary only focusing on {keyword} in the context {section_name}.\n",
    "    ---\n",
    "    Instructions:\n",
    "    1. Do not talk about yourself, only provide the summary of the document.\n",
    "    2. Use only the information provided in the input context. Do not invent new details.\n",
    "    3. The summary needs to be such that it addresses the document with maximum information recall possible within 1000 tokens. \n",
    "    4. Note that this summary is used as an executive summary. So it should be elaborative and pointwise.\n",
    "    Generate the summary accordingly. \n",
    "    ---\n",
    "     \n",
    "    Example Input:\n",
    "    File Name:\n",
    "    'file name or path'\n",
    "    OCR-extracted content:\n",
    "    'OCR-output of document'\n",
    "     \n",
    "    ---\n",
    "    Example Output:\n",
    "    \"summary of document\"\n",
    "    ---\n",
    "    \"\"\"\n",
    "    query_template_qmake = \"\"\"\n",
    "    File Name:\n",
    "    {section_name}\n",
    "    Keyword:\n",
    "    {keyword}\n",
    "    OCR-extracted content:\n",
    "    {chunk}\n",
    "    \"\"\"\n",
    "    system_prompt = systemPrompt_qmake.format(section_name=ticker.replace('.pdf', ' '), keyword=keyword)\n",
    "    query = query_template_qmake.format(section_name=ticker.replace('.pdf', ' '),keyword=keyword, chunk=content)\n",
    "    print(f'Generating summary for {ticker} and {keyword}')\n",
    "    # print(query)\n",
    "    \n",
    "    response = openAIClient_novis.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ], \n",
    "        model=\"onfiGPT-4\",  \n",
    "        temperature=0.15  \n",
    "    )  \n",
    "\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    return summary\n",
    "\n",
    "def save_to_csv(matched_chunks, keyword, filename='final_output.csv', data = []):\n",
    "\n",
    "    for ticker, pages in matched_chunks.items():\n",
    "        all_content = \"\\n\".join(content for content in pages.values())\n",
    "        all_pages = \", \".join(str(pg_no) for pg_no in sorted(pages.keys()))\n",
    "        print(all_pages)\n",
    "        summary = get_summary_pagewise(all_content, ticker, keyword)\n",
    "        \n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            'ticker': ticker,\n",
    "            'keyword': keyword,\n",
    "            'pg_nos': all_pages,\n",
    "            'summary (page_wise)': summary\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "# chunked_slidewise = {\n",
    "#     'MARUTI.pdf': {\n",
    "#         31: \"<<beginning of OCR output>>\\n\\nSlide #31 text:\\npg. no.54\\n<header>    Financial Statements    </header>\\n#    Triple Bottom Line Performance Environment Performance\\nCO2\\n##    CLIMATE CHANGE MANAGEMENT\\nUN SDGS\\n7\\nAFFORDABLE AND CLEAN ENERGY\\n13 ACTION\\nCLIMATE\\nMaterial Issue\\nProducts and Process Emissions Reduction\\n##    Reducing Carbon Emissions from Products: Cumulative CO2 Emissions Avoided\\nThe Company deployed CNG, Smart Hybrid and Intelligent Electric Hybrid (Strong Hybrid) technologies across its products to minimise ca\",\n",
    "#         32: \"In the bustling metropolis of Verdantia, technology and nature coexisted in harmony. Towering skyscrapers, adorned with vertical gardens, reached towards the sky, their glass facades reflecting the vibrant green of rooftop forests. Public transportation was a marvel of efficiency, powered entirely by renewable energy sources. The citizens of Verdantia prided themselves on their innovative spirit and commitment to sustainability. In the heart of the city lay the Grand Plaza, a sprawling park where families gathered, children played, and community events flourished. The plaza was also home to the Verdantia Research Institute, a cutting-edge facility dedicated to developing green technologies and promoting environmental education.\"\n",
    "#     }\n",
    "# }\n",
    "# keyword = \"lithium ion battery\"\n",
    "\n",
    "# save_to_csv(matched_chunked_test, keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # docs_req = ['FLUOROCHEM.pdf', 'KPITTECH.pdf', 'M&M.pdf', 'MARUTI.pdf', 'TATAMOTORS.pdf', 'TVSMOTOR.pdf', 'VOLTAS.pdf']\n",
    "# docs_req = ['BAJAJ-AUTO.pdf']\n",
    "# keyword = \"E Bike\"\n",
    "# keyword = keyword.replace(\" \", \"-\")\n",
    "# for small_doc in docs_req:\n",
    "#     keyword_chunks = get_similar_chunks(small_doc, keyword)\n",
    "#     save_to_csv(keyword_chunks, keyword, filename='new_ll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_keywords = {\n",
    "    # \"APOLLOTYRE.pdf\": [\"EV Battery\"],\n",
    "    # \"BAJAJ-AUTO.pdf\": [\"E Bike\"],\n",
    "    # \"BHARATFORG.pdf\": [\"E Vehicle\", \"E Bike\"],\n",
    "    # \"BPCL.pdf\": [\"Electric Vehicle charger\", \"Electric Vehicle charging facilities\", \"Electric Vehicle charging facility\"],\n",
    "    # \"COCHINSHIP.pdf\": [\"Electric Vehicle charging facilities\", \"Electric Vehicle charging facility\"],\n",
    "    # \"DYNAMATECH.pdf\": [\"E Car\"],\n",
    "    # \"FLUOROCHEM.pdf\": [\"lithium–ion battery\"],\n",
    "    # \"HEROMOTOCO.pdf\": [\"E Scooter\"],\n",
    "    # \"IOC.pdf\": [\"EV Battery\"],\n",
    "    # \"KEI.pdf\": [\"lithium–ion batteries\", \"E Vehicle\"],\n",
    "    # \"KPITTECH.pdf\": [\"EV Charger\", \"Electric Vehicle charger\", \"lithium–ion battery\"],\n",
    "    # \"LTTS.pdf\": [\"E Vehicle\"],\n",
    "    # \"M&M.pdf\": [\"Electric Vehicle charger\", \"electric vehicle related components\", \"E Car\"],\n",
    "    # \"MSUMI.pdf\": [\"electric vehicle related components\"],\n",
    "    # \"NTPC.pdf\": [\"E Bus\", \"E Car\"],\n",
    "    # \"POWERGRID.pdf\": [\"E Vehicle\", \"Electric Vehicle charging facilities\", \"Electric Vehicle charging facility\"],\n",
    "    # \"RELIANCE.pdf\": [\"lithium–ion batteries\", \"EV Battery\"],\n",
    "    # \"ROSSELLIND.pdf\": [\"Society of Indian Defence\"],\n",
    "    # \"SONACOMS.pdf\": [\"EV Battery\"],\n",
    "    # \"TATAMOTORS.pdf\": [\"EV charger components\", \"E Bus\"],\n",
    "    \"TATAPOWER.pdf\": [\"Electric Vehicle charger\", \"E Bus\", \"Electric Vehicle charging facility\"],\n",
    "    \"TVSMOTOR.pdf\": [\"E Scooter\", \"E Bike\"],\n",
    "    \"UNOMINDA.pdf\": [\"Electric Vehicle charger\"],\n",
    "    \"VOLTAS.pdf\": [\"lithium–ion battery\", \"Electric Vehicle charging facility\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('TATAPOWER.pdf', ['Electric Vehicle charger', 'E Bus', 'Electric Vehicle charging facility']), ('TVSMOTOR.pdf', ['E Scooter', 'E Bike']), ('UNOMINDA.pdf', ['Electric Vehicle charger']), ('VOLTAS.pdf', ['lithium–ion battery', 'Electric Vehicle charging facility'])])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_keywords.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got result\n",
      "Generating summary for TATAPOWER.pdf and Electric Vehicle charger\n",
      "got result\n",
      "Generating summary for TATAPOWER.pdf and E Bus\n",
      "got result\n",
      "Generating summary for TATAPOWER.pdf and Electric Vehicle charging facility\n",
      "got result\n",
      "Generating summary for TVSMOTOR.pdf and E Scooter\n",
      "got result\n",
      "Generating summary for TVSMOTOR.pdf and E Bike\n",
      "got result\n",
      "Generating summary for UNOMINDA.pdf and Electric Vehicle charger\n",
      "got result\n",
      "Generating summary for VOLTAS.pdf and lithium–ion battery\n",
      "got result\n",
      "Generating summary for VOLTAS.pdf and Electric Vehicle charging facility\n"
     ]
    }
   ],
   "source": [
    "for doc_name, keywords in documents_keywords.items():\n",
    "    for keyword in keywords:\n",
    "            keyword_chunks = get_similar_chunks(doc_name, keyword)\n",
    "            save_to_csv(keyword_chunks, keyword, filename='./csvs/final_new2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nse-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
